{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from barbar import Bar\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Class\n",
    "\n",
    "class TrivialDataset(Dataset):\n",
    "\t\"\"\"The S_trivial set: only one bird vocalizing, no radio noise.\"\"\"\n",
    "\n",
    "\tdef __init__(self, all_facc, all_mic):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\troot_dir (string): Directory with all the extracted samples.\n",
    "\t\t\"\"\"\n",
    "\t\tself.all_facc = all_facc\n",
    "\t\tself.all_mic = all_mic\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.all_facc) # or use length of mic recordings, they are the same\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tif torch.is_tensor(idx):\n",
    "\t\t\tidx = idx.tolist()\n",
    "\n",
    "\t\tfacc = self.all_facc[idx]\n",
    "\t\tmic = self.all_mic[idx]\n",
    "\n",
    "\t\tsample = {'facc': facc, 'mic': mic}\n",
    "\n",
    "\t\treturn sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear regression model, take in input/output size at initialization\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "DATA_SIZE = 333\n",
    "TRAIN_SIZE = 300\n",
    "max_epochs = 50\n",
    "n = 2 # timesteps taken as input to fit one step of output\n",
    "input_dim = 372*n # number of magnitudes taken as input (in acc_female)\n",
    "output_dim = 372 # number of magnitudes to fit as output (microphone)\n",
    "LR = 1e-4 # learning rate, can be changed\n",
    "BS = 16 # batch size, can be changed\n",
    "lengthfull = 513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(input_dim, output_dim)\n",
    "# For GPU only\n",
    "# if torch.cuda.is_available():\n",
    "#     model = LinearRegression(input_dim, output_dim).cuda(0)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check location of S_trivial storage\n",
    "fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "print(fileDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load separate files and combine alltogether\n",
    "# The files are in the form of compressed numpy arrays, each when loaded first comes as a dictionary\n",
    "# Keys in the dictionary: 'mic' & 'female_acc' are the names of data channel\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "NumSamples = 333\n",
    "\n",
    "for i in range(NumSamples):\n",
    "    filename = os.path.join(fileDir, 'datat/trivial_sample_' + str(i) +'.npz')\n",
    "    specs = np.load(filename)\n",
    "    micr = np.transpose(specs['mic'],(1,0))\n",
    "    facc = np.transpose(specs['female_acc'],(1,0))\n",
    "    X.append(facc)\n",
    "    Y.append(micr)\n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# trivial = TrivialDataset(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(np.arange(0, DATA_SIZE))\n",
    "train_indices = indices[:TRAIN_SIZE]\n",
    "valid_indices = indices[TRAIN_SIZE:]\n",
    "trainset = TrivialDataset(X[train_indices], Y[train_indices])\n",
    "trainloader = DataLoader(trainset, batch_size=BS, shuffle=True)\n",
    "validset = TrivialDataset(X[valid_indices], Y[valid_indices])\n",
    "validloader = DataLoader(validset, batch_size=1, shuffle=False)\n",
    "\n",
    "datasets, dataloaders = {'train': trainset, 'valid':validset}, {'train': trainloader, 'valid':validloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "phases = ['train', 'valid']\n",
    "\n",
    "for e in range(max_epochs):\n",
    "    epoch_loss_train = []\n",
    "    epoch_loss_valid = []\n",
    "    \n",
    "    for phase in phases:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for i_batch, sample_batched in enumerate(Bar(dataloaders[phase])):\n",
    "            # data is in shape (batchsize, freq_magnitude_size, timesteps)\n",
    "            batched_acc = sample_batched['facc']\n",
    "            batched_mic = sample_batched['mic']\n",
    "                \n",
    "            for timestep in range(lengthfull):\n",
    "                segment_acc = batched_acc[:, :, timestep]\n",
    "                segment_mic = batched_mic[:, :, timestep]\n",
    "                for j in range(1, n):\n",
    "                    curr_pos = (timestep + j) % lengthfull\n",
    "                    segment_acc = np.concatenate((segment_acc, batched_acc[:, :, curr_pos]), axis=1)\n",
    "            \n",
    "                series = Variable(torch.from_numpy(segment_acc).float())\n",
    "                target = Variable(segment_mic.float())\n",
    "            \n",
    "                outputs = model(series)\n",
    "                loss = criterion(outputs, target)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    epoch_loss_train.append(loss.data)\n",
    "                else:\n",
    "                    epoch_loss_valid.append(loss.data)\n",
    "    \n",
    "    \n",
    "    # print progress metric\n",
    "    eploss_train = torch.mean(torch.stack(epoch_loss_train))\n",
    "    train_loss.append(eploss_train)\n",
    "    eploss_valid = torch.mean(torch.stack(epoch_loss_valid))\n",
    "    valid_loss.append(eploss_valid)\n",
    "    \n",
    "    progress_str = '[epoch {}/{}] - Train Loss: {:.4f} Valid Loss: {:.4f}'.format(e + 1, max_epochs, eploss_train, eploss_valid)\n",
    "    print(progress_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
